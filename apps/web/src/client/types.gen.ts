// This file is auto-generated by @hey-api/openapi-ts

/**
 * Controls the processing and generation for the segment.
 * - `crop_image` controls whether to crop the file's images to the segment's bounding box.
 * The cropped image will be stored in the segment's `image` field. Use `All` to always crop,
 * or `Auto` to only crop when needed for post-processing.
 * - `html` is the HTML output for the segment, generated either through heuristic (`Auto`) or using Chunkr fine-tuned models (`LLM`)
 * - `llm` is the LLM-generated output for the segment, this uses off-the-shelf models to generate a custom output for the segment
 * - `markdown` is the Markdown output for the segment, generated either through heuristic (`Auto`) or using Chunkr fine-tuned models (`LLM`)
 * - `embed_sources` defines which content sources will be included in the chunk's embed field and counted towards the chunk length.
 * The array's order determines the sequence in which content appears in the embed field (e.g., [Markdown, LLM] means Markdown content
 * is followed by LLM content). This directly affects what content is available for embedding and retrieval.
 */
export type AutoGenerationConfig = {
  crop_image?: CroppingStrategy
  embed_sources?: Array<EmbedSource>
  /**
   * Use the full page image as context for LLM generation
   */
  extended_context?: boolean
  html?: GenerationStrategy
  llm?: string | null
  markdown?: GenerationStrategy
}

/**
 * Bounding box for an item. It is used for chunks, segments and OCR results.
 */
export type BoundingBox = {
  /**
   * The height of the bounding box.
   */
  height: number
  /**
   * The left coordinate of the bounding box.
   */
  left: number
  /**
   * The top coordinate of the bounding box.
   */
  top: number
  /**
   * The width of the bounding box.
   */
  width: number
}

export type Chunk = {
  /**
   * The unique identifier for the chunk.
   */
  chunk_id?: string
  /**
   * The total number of tokens in the chunk. Calculated by the `tokenizer`.
   */
  chunk_length: number
  /**
   * Suggested text to be embedded for the chunk. This text is generated by combining the embed content
   * from each segment according to the configured embed sources (HTML, Markdown, LLM, or Content).
   * Can be configured using `embed_sources` in the `SegmentProcessing` configuration.
   */
  embed?: string | null
  /**
   * Collection of document segments that form this chunk.
   * When `target_chunk_length` > 0, contains the maximum number of segments
   * that fit within that length (segments remain intact).
   * Otherwise, contains exactly one segment.
   */
  segments: Array<Segment>
}

/**
 * Controls the setting for the chunking and post-processing of each chunk.
 */
export type ChunkProcessing = {
  /**
   * Whether to ignore headers and footers in the chunking process.
   * This is recommended as headers and footers break reading order across pages.
   */
  ignore_headers_and_footers?: boolean
  /**
   * The target number of words in each chunk. If 0, each chunk will contain a single segment.
   */
  target_length?: number
  /**
   * The tokenizer to use for the chunking process.
   */
  tokenizer?: TokenizerType
}

export type Configuration = {
  chunk_processing: ChunkProcessing
  error_handling: ErrorHandlingStrategy
  /**
   * The number of seconds until task is deleted.
   * Expired tasks can **not** be updated, polled or accessed via web interface.
   */
  expires_in?: number | null
  /**
   * Whether to use high-resolution images for cropping and post-processing.
   */
  high_resolution: boolean
  /**
   * The presigned URL of the input file.
   */
  input_file_url?: string | null
  /**
   * @deprecated
   */
  json_schema?: unknown
  llm_processing: LlmProcessing
  model?: null | Model
  ocr_strategy: OcrStrategy
  pipeline?: null | PipelineType
  segment_processing: SegmentProcessing
  segmentation_strategy: SegmentationStrategy
  /**
   * The target number of words in each chunk. If 0, each chunk will contain a single segment.
   * @deprecated
   */
  target_chunk_length?: number | null
}

export type CreateForm = {
  chunk_processing?: null | ChunkProcessing
  error_handling?: null | ErrorHandlingStrategy
  /**
   * The number of seconds until task is deleted.
   * Expired tasks can **not** be updated, polled or accessed via web interface.
   */
  expires_in?: number | null
  /**
   * The file to be uploaded. Can be a URL or a base64 encoded file.
   */
  file: string
  /**
   * The name of the file to be uploaded. If not set a name will be generated.
   */
  file_name?: string | null
  /**
   * Whether to use high-resolution images for cropping and post-processing. (Latency penalty: ~7 seconds per page)
   */
  high_resolution?: boolean | null
  llm_processing?: null | LlmProcessing
  ocr_strategy?: null | OcrStrategy
  pipeline?: null | PipelineType
  segment_processing?: null | SegmentProcessing
  segmentation_strategy?: null | SegmentationStrategy
}

export type CreateFormMultipart = {
  chunk_processing?: null | ChunkProcessing
  /**
   * The number of seconds until task is deleted.
   * Expired tasks can **not** be updated, polled or accessed via web interface.
   */
  expires_in?: number | null
  /**
   * The file to be uploaded.
   */
  file: Blob | File
  /**
   * Whether to use high-resolution images for cropping and post-processing. (Latency penalty: ~7 seconds per page)
   */
  high_resolution?: boolean | null
  ocr_strategy?: null | OcrStrategy
  pipeline?: null | PipelineType
  segment_processing?: null | SegmentProcessing
  segmentation_strategy?: null | SegmentationStrategy
}

/**
 * Controls the cropping strategy for an item (e.g. segment, chunk, etc.)
 * - `All` crops all images in the item
 * - `Auto` crops images only if required for post-processing
 */
export type CroppingStrategy = 'All' | 'Auto'

export type EmbedSource = 'HTML' | 'Markdown' | 'LLM' | 'Content'

/**
 * Controls how errors are handled during processing:
 * - `Fail`: Stops processing and fails the task when any error occurs
 * - `Continue`: Attempts to continue processing despite non-critical errors (eg. LLM refusals etc.)
 */
export type ErrorHandlingStrategy = 'Fail' | 'Continue'

/**
 * Specifies the fallback strategy for LLM processing
 *
 * This can be:
 * 1. None - No fallback will be used
 * 2. Default - The system default fallback model will be used
 * 3. Model - A specific model ID will be used as fallback (check the documentation for the models.)
 */
export type FallbackStrategy =
  | 'None'
  | 'Default'
  | {
      /**
       * Use a specific model as fallback
       */
      Model: string
    }

export type GenerationStrategy = 'LLM' | 'Auto'

/**
 * Controls the processing and generation for the segment.
 * - `crop_image` controls whether to crop the file's images to the segment's bounding box.
 * The cropped image will be stored in the segment's `image` field. Use `All` to always crop,
 * or `Auto` to only crop when needed for post-processing.
 * - `html` is the HTML output for the segment, generated either through huerstics (`Auto`) or using Chunkr fine-tuned models (`LLM`)
 * - `llm` is the LLM-generated output for the segment, this uses off-the-shelf models to generate a custom output for the segment
 * - `markdown` is the Markdown output for the segment, generated either through huerstics (`Auto`) or using Chunkr fine-tuned models (`LLM`)
 * - `embed_sources` defines which content sources will be included in the chunk's embed field and counted towards the chunk length.
 * The array's order determines the sequence in which content appears in the embed field (e.g., [Markdown, LLM] means Markdown content
 * is followed by LLM content). This directly affects what content is available for embedding and retrieval.
 */
export type LlmGenerationConfig = {
  crop_image?: CroppingStrategy
  embed_sources?: Array<EmbedSource>
  /**
   * Use the full page image as context for LLM generation
   */
  extended_context?: boolean
  html?: GenerationStrategy
  /**
   * Prompt for the LLM model
   */
  llm?: string | null
  markdown?: GenerationStrategy
}

/**
 * Controls the LLM used for the task.
 */
export type LlmProcessing = {
  /**
   * The fallback strategy to use for the LLMs in the task.
   */
  fallback_strategy?: FallbackStrategy
  /**
   * The maximum number of tokens to generate.
   */
  max_completion_tokens?: number | null
  /**
   * The ID of the model to use for the task. If not provided, the default model will be used.
   * Please check the documentation for the model you want to use.
   */
  model_id?: string | null
  /**
   * The temperature to use for the LLM.
   */
  temperature?: number
}

/**
 * @deprecated
 */
export type Model = 'Fast' | 'HighQuality'

/**
 * OCR results for a segment
 */
export type OcrResult = {
  bbox: BoundingBox
  /**
   * The confidence score of the recognized text.
   */
  confidence?: number | null
  /**
   * The recognized text of the OCR result.
   */
  text: string
}

/**
 * Controls the Optical Character Recognition (OCR) strategy.
 * - `All`: Processes all pages with OCR. (Latency penalty: ~0.5 seconds per page)
 * - `Auto`: Selectively applies OCR only to pages with missing or low-quality text. When text layer is present the bounding boxes from the text layer are used.
 */
export type OcrStrategy = 'All' | 'Auto'

/**
 * The processed results of a document analysis task
 */
export type OutputResponse = {
  /**
   * Collection of document chunks, where each chunk contains one or more segments
   */
  chunks: Array<Chunk>
  /**
   * The extracted JSON from the document.
   * @deprecated
   */
  extracted_json?: unknown
  /**
   * The name of the file.
   */
  file_name?: string | null
  /**
   * The number of pages in the file.
   */
  page_count?: number | null
  /**
   * The presigned URL of the PDF file.
   */
  pdf_url?: string | null
}

/**
 * Controls the cropping strategy for an item (e.g. segment, chunk, etc.)
 * - `All` crops all images in the item
 * - `Auto` crops images only if required for post-processing
 */
export type PictureCroppingStrategy = 'All' | 'Auto'

/**
 * Controls the processing and generation for the segment.
 * - `crop_image` controls whether to crop the file's images to the segment's bounding box.
 * The cropped image will be stored in the segment's `image` field. Use `All` to always crop,
 * or `Auto` to only crop when needed for post-processing.
 * - `html` is the HTML output for the segment, generated either through heuristic (`Auto`) or using Chunkr fine-tuned models (`LLM`)
 * - `llm` is the LLM-generated output for the segment, this uses off-the-shelf models to generate a custom output for the segment
 * - `markdown` is the Markdown output for the segment, generated either through heuristic (`Auto`) or using Chunkr fine-tuned models (`LLM`)
 * - `embed_sources` defines which content sources will be included in the chunk's embed field and counted towards the chunk length.
 * The array's order determines the sequence in which content appears in the embed field (e.g., [Markdown, LLM] means Markdown content
 * is followed by LLM content). This directly affects what content is available for embedding and retrieval.
 */
export type PictureGenerationConfig = {
  crop_image?: PictureCroppingStrategy
  embed_sources?: Array<EmbedSource>
  /**
   * Use the full page image as context for LLM generation
   */
  extended_context?: boolean
  html?: GenerationStrategy
  /**
   * Prompt for the LLM model
   */
  llm?: string | null
  markdown?: GenerationStrategy
}

export type PipelineType = 'Azure' | 'Chunkr'

export type Segment = {
  bbox: BoundingBox
  /**
   * Confidence score of the layout analysis model
   */
  confidence?: number | null
  /**
   * Text content of the segment. Calculated by the OCR results.
   */
  content?: string
  /**
   * HTML representation of the segment.
   */
  html?: string
  /**
   * Presigned URL to the image of the segment.
   */
  image?: string | null
  /**
   * LLM representation of the segment.
   */
  llm?: string | null
  /**
   * Markdown representation of the segment.
   */
  markdown?: string
  /**
   * OCR results for the segment.
   */
  ocr?: Array<OcrResult> | null
  /**
   * Height of the page containing the segment.
   */
  page_height: number
  /**
   * Page number of the segment.
   */
  page_number: number
  /**
   * Width of the page containing the segment.
   */
  page_width: number
  /**
   * Unique identifier for the segment.
   */
  segment_id: string
  segment_type: SegmentType
}

/**
 * Controls the post-processing of each segment type.
 * Allows you to generate HTML and Markdown from chunkr models for each segment type.
 * By default, the HTML and Markdown are generated manually using the segmentation information except for `Table`, `Formula` and `Picture`.
 * You can optionally configure custom LLM prompts and models to generate an additional `llm` field with LLM-processed content for each segment type.
 *
 * The configuration of which content sources (HTML, Markdown, LLM, Content) of the segment
 * should be included in the chunk's `embed` field and counted towards the chunk length can be configured through the `embed_sources` setting.
 */
export type SegmentProcessing = {
  Caption?: null | AutoGenerationConfig
  Footnote?: null | AutoGenerationConfig
  Formula?: null | LlmGenerationConfig
  ListItem?: null | AutoGenerationConfig
  Page?: null | LlmGenerationConfig
  PageFooter?: null | AutoGenerationConfig
  PageHeader?: null | AutoGenerationConfig
  Picture?: null | PictureGenerationConfig
  SectionHeader?: null | AutoGenerationConfig
  Table?: null | LlmGenerationConfig
  Text?: null | AutoGenerationConfig
  Title?: null | AutoGenerationConfig
}

/**
 * All the possible types for a segment.
 * Note: Different configurations will produce different types.
 * Please refer to the documentation for more information.
 */
export type SegmentType =
  | 'Caption'
  | 'Footnote'
  | 'Formula'
  | 'ListItem'
  | 'Page'
  | 'PageFooter'
  | 'PageHeader'
  | 'Picture'
  | 'SectionHeader'
  | 'Table'
  | 'Text'
  | 'Title'

/**
 * Controls the segmentation strategy:
 * - `LayoutAnalysis`: Analyzes pages for layout elements (e.g., `Table`, `Picture`, `Formula`, etc.) using bounding boxes. Provides fine-grained segmentation and better chunking. (Latency penalty: ~TBD seconds per page).
 * - `Page`: Treats each page as a single segment. Faster processing, but without layout element detection and only simple chunking.
 */
export type SegmentationStrategy = 'LayoutAnalysis' | 'Page'

/**
 * The status of the task.
 */
export type Status =
  | 'Starting'
  | 'Processing'
  | 'Succeeded'
  | 'Failed'
  | 'Cancelled'

export type TaskResponse = {
  configuration: Configuration
  /**
   * The date and time when the task was created and queued.
   */
  created_at: string
  /**
   * The date and time when the task will expire.
   */
  expires_at?: string | null
  /**
   * The date and time when the task was finished.
   */
  finished_at?: string | null
  /**
   * A message describing the task's status or any errors that occurred.
   */
  message: string
  output?: null | OutputResponse
  /**
   * The date and time when the task was started.
   */
  started_at?: string | null
  status: Status
  /**
   * The unique identifier for the task.
   */
  task_id: string
  /**
   * The presigned URL of the task.
   */
  task_url?: string | null
}

/**
 * Common tokenizers used for text processing.
 *
 * These values represent standard tokenization approaches and popular pre-trained
 * tokenizers from the Hugging Face ecosystem.
 */
export type Tokenizer =
  | 'Word'
  | 'Cl100kBase'
  | 'XlmRobertaBase'
  | 'BertBaseUncased'

/**
 * Specifies which tokenizer to use for the chunking process.
 *
 * This type supports two ways of specifying a tokenizer:
 * 1. Using a predefined tokenizer from the `Tokenizer` enum
 * 2. Using any Hugging Face tokenizer by providing its model ID as a string
 * (e.g. "facebook/bart-large", "Qwen/Qwen-tokenizer", etc.)
 *
 * When using a string, any valid Hugging Face tokenizer ID can be specified,
 * which will be loaded using the Hugging Face tokenizers library.
 */
export type TokenizerType =
  | {
      /**
       * Use one of the predefined tokenizer types
       */
      Enum: Tokenizer
    }
  | {
      /**
       * Use any Hugging Face tokenizer by specifying its model ID
       * Examples: "Qwen/Qwen-tokenizer", "facebook/bart-large"
       */
      String: string
    }

export type UpdateForm = {
  chunk_processing?: null | ChunkProcessing
  error_handling?: null | ErrorHandlingStrategy
  /**
   * The number of seconds until task is deleted.
   * Expired tasks can **not** be updated, polled or accessed via web interface.
   */
  expires_in?: number | null
  /**
   * Whether to use high-resolution images for cropping and post-processing. (Latency penalty: ~7 seconds per page)
   */
  high_resolution?: boolean | null
  llm_processing?: null | LlmProcessing
  ocr_strategy?: null | OcrStrategy
  pipeline?: null | PipelineType
  segment_processing?: null | SegmentProcessing
  segmentation_strategy?: null | SegmentationStrategy
}

export type UpdateFormMultipart = {
  chunk_processing?: null | ChunkProcessing
  /**
   * The number of seconds until task is deleted.
   * Expried tasks can **not** be updated, polled or accessed via web interface.
   */
  expires_in?: number | null
  /**
   * Whether to use high-resolution images for cropping and post-processing. (Latency penalty: ~7 seconds per page)
   */
  high_resolution?: boolean | null
  ocr_strategy?: null | OcrStrategy
  pipeline?: null | PipelineType
  segment_processing?: null | SegmentProcessing
  segmentation_strategy?: null | SegmentationStrategy
}

export type CreateTaskRouteData = {
  /**
   * JSON request to create a task
   */
  body: CreateForm
  path?: never
  query?: never
  url: '/api/v1/task/parse'
}

export type CreateTaskRouteErrors = {
  /**
   * Internal server error related to creating the task
   */
  500: string
}

export type CreateTaskRouteError =
  CreateTaskRouteErrors[keyof CreateTaskRouteErrors]

export type CreateTaskRouteResponses = {
  /**
   * Detailed information describing the task, its status and processed outputs
   */
  200: TaskResponse
}

export type CreateTaskRouteResponse =
  CreateTaskRouteResponses[keyof CreateTaskRouteResponses]

export type DeleteTaskRouteData = {
  body?: never
  path: {
    /**
     * Id of the task to delete
     */
    task_id: string | null
  }
  query?: never
  url: '/api/v1/task/{task_id}'
}

export type DeleteTaskRouteErrors = {
  /**
   * Internal server error related to deleting the task
   */
  500: string
}

export type DeleteTaskRouteError =
  DeleteTaskRouteErrors[keyof DeleteTaskRouteErrors]

export type DeleteTaskRouteResponses = {
  /**
   * Task deleted successfully
   */
  200: unknown
}

export type GetTaskRouteData = {
  body?: never
  path: {
    /**
     * Id of the task to retrieve
     */
    task_id: string | null
  }
  query?: {
    /**
     * Whether to return base64 encoded URLs. If false, the URLs will be returned as presigned URLs.
     */
    base64_urls?: boolean
    /**
     * Whether to include chunks in the output response
     */
    include_chunks?: boolean
  }
  url: '/api/v1/task/{task_id}'
}

export type GetTaskRouteErrors = {
  /**
   * Internal server error related to getting the task
   */
  500: string
}

export type GetTaskRouteError = GetTaskRouteErrors[keyof GetTaskRouteErrors]

export type GetTaskRouteResponses = {
  /**
   * Detailed information describing the task
   */
  200: TaskResponse
}

export type GetTaskRouteResponse =
  GetTaskRouteResponses[keyof GetTaskRouteResponses]

export type CancelTaskRouteData = {
  body?: never
  path: {
    /**
     * Id of the task to cancel
     */
    task_id: string | null
  }
  query?: never
  url: '/api/v1/task/{task_id}/cancel'
}

export type CancelTaskRouteErrors = {
  /**
   * Internal server error related to cancelling the task
   */
  500: string
}

export type CancelTaskRouteError =
  CancelTaskRouteErrors[keyof CancelTaskRouteErrors]

export type CancelTaskRouteResponses = {
  /**
   * Task cancelled successfully
   */
  200: unknown
}

export type UpdateTaskRouteData = {
  /**
   * JSON request to update an task
   */
  body: UpdateForm
  path: {
    task_id: string
  }
  query?: never
  url: '/api/v1/task/{task_id}/parse'
}

export type UpdateTaskRouteErrors = {
  /**
   * Internal server error related to updating the task
   */
  500: string
}

export type UpdateTaskRouteError =
  UpdateTaskRouteErrors[keyof UpdateTaskRouteErrors]

export type UpdateTaskRouteResponses = {
  /**
   * Detailed information describing the task, its status and processed outputs
   */
  200: TaskResponse
}

export type UpdateTaskRouteResponse =
  UpdateTaskRouteResponses[keyof UpdateTaskRouteResponses]

export type GetTasksRouteData = {
  body?: never
  path?: never
  query?: {
    /**
     * Whether to return base64 encoded URLs. If false, the URLs will be returned as presigned URLs.
     */
    base64_urls?: boolean
    /**
     * End date
     */
    end?: string
    /**
     * Whether to include chunks in the output response
     */
    include_chunks?: boolean
    /**
     * Number of tasks per page
     */
    limit?: number
    /**
     * Page number
     */
    page?: number
    /**
     * Start date
     */
    start?: string
  }
  url: '/api/v1/tasks'
}

export type GetTasksRouteErrors = {
  /**
   * Internal server error related to getting the task
   */
  500: string
}

export type GetTasksRouteError = GetTasksRouteErrors[keyof GetTasksRouteErrors]

export type GetTasksRouteResponses = {
  /**
   * Detailed information describing the task
   */
  200: Array<TaskResponse>
}

export type GetTasksRouteResponse =
  GetTasksRouteResponses[keyof GetTasksRouteResponses]

export type HealthCheckData = {
  body?: never
  path?: never
  query?: never
  url: '/health'
}

export type HealthCheckResponses = {
  /**
   * Confirmation that the service can respond to requests and the version of the service
   */
  200: string
}

export type HealthCheckResponse =
  HealthCheckResponses[keyof HealthCheckResponses]

export type ClientOptions = {
  baseUrl: 'https://api.chunkr.ai' | (string & {})
}
